{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cbb9f2c",
   "metadata": {},
   "source": [
    "second: rotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "552656d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching up to 200 new posts from r/StartUpIndia...\n",
      "Database contains 5909 existing posts\n",
      "\n",
      "Checking 'top year' posts...\n",
      "\n",
      "Checking 'top month' posts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HOME\\AppData\\Local\\Temp\\ipykernel_22768\\994726173.py:94: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  'fetched_at': datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking 'top week' posts...\n",
      "Found 10 new posts (processed 2101 total)\n",
      "Found 20 new posts (processed 2215 total)\n",
      "\n",
      "Checking 'top day' posts...\n",
      "\n",
      "Checking 'top all' posts...\n",
      "\n",
      "Checking 'top hour' posts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HOME\\AppData\\Local\\Temp\\ipykernel_22768\\994726173.py:127: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"last_updated\": datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully added 22 new posts\n",
      "Total posts in database: 5931\n",
      "Posts processed this run: 3242\n",
      "Data saved to startupindia_posts_comments.json\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Initialize Reddit API\n",
    "reddit = praw.Reddit(\n",
    "    client_id=' ',\n",
    "    client_secret=' ',\n",
    "    user_agent=' '\n",
    ")\n",
    "\n",
    "# Constants\n",
    "SUBREDDIT = \"StartUpIndia\"\n",
    "# SUBREDDIT = \"indianstartups\"\n",
    "POST_LIMIT = 200\n",
    "COMMENT_LIMIT = 3\n",
    "DATA_FILE = \"startupindia_posts_comments.json\"\n",
    "MAX_POSTS_TO_CHECK = 10000  # Increased from 1000\n",
    "TIME_FILTERS = ['year', 'month', 'week', 'day', 'all', 'hour']  # Multiple time filters\n",
    "\n",
    "def load_existing_data():\n",
    "    \"\"\"Load existing data with OrderedDict for efficient pruning\"\"\"\n",
    "    if os.path.exists(DATA_FILE):\n",
    "        with open(DATA_FILE, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            # Convert posts to OrderedDict to maintain insertion order\n",
    "            data['posts'] = OrderedDict(\n",
    "                sorted(data['posts'].items(), \n",
    "                      key=lambda x: x[1]['fetched_at'], \n",
    "                      reverse=True)\n",
    "            )\n",
    "            return data\n",
    "    return {\"posts\": OrderedDict(), \"last_updated\": None}\n",
    "\n",
    "def get_top_comments(submission):\n",
    "    \"\"\"Fetch top comments with error handling\"\"\"\n",
    "    try:\n",
    "        submission.comments.replace_more(limit=0)\n",
    "        return [{\n",
    "            'id': comment.id,\n",
    "            'author': str(comment.author),\n",
    "            'body': comment.body,\n",
    "            'score': comment.score,\n",
    "            'created_utc': comment.created_utc,\n",
    "            'permalink': f\"https://reddit.com{comment.permalink}\"\n",
    "        } for comment in submission.comments[:COMMENT_LIMIT]]\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting comments for {submission.id}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def fetch_with_time_filter(time_filter):\n",
    "    \"\"\"Generator that yields posts with a given time filter\"\"\"\n",
    "    for post in reddit.subreddit(SUBREDDIT).top(time_filter=time_filter, limit=None):\n",
    "        yield post\n",
    "\n",
    "def main():\n",
    "    print(f\"Fetching up to {POST_LIMIT} new posts from r/{SUBREDDIT}...\")\n",
    "    \n",
    "    # Load existing data\n",
    "    existing_data = load_existing_data()\n",
    "    existing_posts = existing_data['posts']\n",
    "    existing_ids = set(existing_posts.keys())\n",
    "    print(f\"Database contains {len(existing_posts)} existing posts\")\n",
    "    \n",
    "    new_posts = OrderedDict()\n",
    "    total_processed = 0\n",
    "    found_new = 0\n",
    "    \n",
    "    # Strategy: Try different time filters to find new posts\n",
    "    for time_filter in TIME_FILTERS:\n",
    "        if found_new >= POST_LIMIT:\n",
    "            break\n",
    "            \n",
    "        print(f\"\\nChecking 'top {time_filter}' posts...\")\n",
    "        try:\n",
    "            for post in fetch_with_time_filter(time_filter):\n",
    "                total_processed += 1\n",
    "                \n",
    "                if post.id not in existing_ids:\n",
    "                    post_data = {\n",
    "                        'id': post.id,\n",
    "                        'title': post.title,\n",
    "                        'author': str(post.author),\n",
    "                        'score': post.score,\n",
    "                        'upvote_ratio': post.upvote_ratio,\n",
    "                        'num_comments': post.num_comments,\n",
    "                        'created_utc': post.created_utc,\n",
    "                        'url': post.url,\n",
    "                        'selftext': post.selftext,\n",
    "                        'permalink': f\"https://reddit.com{post.permalink}\",\n",
    "                        'fetched_at': datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                        'time_filter': time_filter,\n",
    "                        'comments': get_top_comments(reddit.submission(id=post.id))\n",
    "                    }\n",
    "                    new_posts[post.id] = post_data\n",
    "                    existing_ids.add(post.id)\n",
    "                    found_new += 1\n",
    "                    \n",
    "                    if found_new % 10 == 0:\n",
    "                        print(f\"Found {found_new} new posts (processed {total_processed} total)\")\n",
    "                    \n",
    "                    if found_new >= POST_LIMIT or total_processed >= MAX_POSTS_TO_CHECK:\n",
    "                        break\n",
    "                \n",
    "                # Rate limiting\n",
    "                if total_processed % 100 == 0:\n",
    "                    time.sleep(2)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {time_filter} posts: {str(e)}\")\n",
    "    \n",
    "    if not new_posts:\n",
    "        print(f\"No new posts found after checking {total_processed} posts across {len(TIME_FILTERS)} time periods\")\n",
    "        return\n",
    "    \n",
    "    # Merge and limit total posts to reasonable size\n",
    "    merged_posts = OrderedDict(list(new_posts.items()) + list(existing_posts.items()))\n",
    "    # if len(merged_posts) > 10000:  # Keep only most recent 5000 posts\n",
    "    #     merged_posts = OrderedDict(list(merged_posts.items())[:5000])\n",
    "    \n",
    "    # Prepare final data\n",
    "    updated_data = {\n",
    "        \"posts\": merged_posts,\n",
    "        \"last_updated\": datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        \"stats\": {\n",
    "            \"total_posts\": len(merged_posts),\n",
    "            \"new_posts_added\": len(new_posts),\n",
    "            \"posts_processed\": total_processed,\n",
    "            \"time_filters_used\": TIME_FILTERS\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save data\n",
    "    with open(DATA_FILE, 'w', encoding='utf-8') as f:\n",
    "        json.dump(updated_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\nSuccessfully added {len(new_posts)} new posts\")\n",
    "    print(f\"Total posts in database: {len(merged_posts)}\")\n",
    "    print(f\"Posts processed this run: {total_processed}\")\n",
    "    print(f\"Data saved to {DATA_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129286e7",
   "metadata": {},
   "source": [
    "third: hot, new, top"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
